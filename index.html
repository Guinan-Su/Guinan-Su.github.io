<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
<!-- Please delete this script if you use this HTML. -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-143495247-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-143495247-1');
</script>

<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=500">
  <link href="files/stylesheet.css" rel="stylesheet" type="text/css">
  <title>Yanwu Yang (杨延武) - Personal Site</title>
  
  <link href="files/css" rel="stylesheet" type="text/css">
</head>

<body>
  <table width="950" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody><tr>
            <td width="5%">
            </td> 
            <td width="30%">
              <img src="files/photo.png" height="256">
            </td>
            <td width="65%" valign="middle">
              <p align="center">
                <name>Yanwu Yang (杨延武)</name>
              </p>
              <p align="center">
                <img src="files/email_icon.png" width="21" align="absmiddle">
<a href="mailto:yangyanwu1111@gmail.com">yangyanwu1111@gmail.com</a>
              <p>
                I am now a PhD candidate studying in Harbin Institute of Technology at Shenzhen, as well as an intern in Peng Cheng National Lab. My interests are in medical image processing, brain networks for diagnosis, and neuroscience studies. </p>

<br> My research interests are in computer vision and deep learning.
              <p align="left">
                <a href="https://scholar.google.com/citations?user=3q8-ym8AAAAJ">Google Scholar</a>
                <a href="https://github.com/podismine">Github</a>
              </p>
            </td>
          </tr>
        </tbody>
        </table>
        <heading>News</heading><p>
        <ul style="list-style-type:circle;">
        <li style="margin: 8px 0;">[07.2023] One paper (First Author) is accepted by <a href="https://www.sciencedirect.com/science/article/pii/S1361841523001767">MedIA</a>.
        <li style="margin: 8px 0;">[07.2023] One paper (First Author) is accepted by <a href="https://ieeexplore.ieee.org/abstract/document/10182318">TMI</a>.
        <li style="margin: 8px 0;">[04.2023] One paper (First Author) is accepted by <a href="https://www.sciencedirect.com/science/article/pii/S0893608023002113">Neural Networks</a>.
        <li style="margin: 8px 0;">[03.2023] One paper (First Author) is accepted by <a href="https://openreview.net/forum?id=WjrcYNTPunQ">MIDL-2023</a>.
        <li style="margin: 8px 0;">[02.2023] One paper (First Author) is accepted by <a href="https://ieeexplore.ieee.org/abstract/document/10095707">ICASSP-2023</a> (oral).
        <li style="margin: 8px 0;">[02.2023] Two papers (Other Author) are accepted by ISBI-2023.
        <li style="margin: 8px 0;">[02.2023] We are awarded the <strong>Open Science Excellent Author Program</strong> by Wiley.
        <li style="margin: 8px 0;">[09.2022] One paper (Co-First Author) is accepted by <a href="https://onlinelibrary.wiley.com/doi/pdf/10.1002/hbm.26066">Human Brain Mapping</a>.
        <li style="margin: 8px 0;">[08.2022] One paper (First Author) is accepted by <a href="https://ieeexplore.ieee.org/abstract/document/9995642">BIBM-2022</a> (oral).
        <li style="margin: 8px 0;">[07.2022] One paper (Co-First Author) is accepted by <a href="https://www.frontiersin.org/articles/10.3389/fnins.2022.940381/full">Front. in Neuroscience</a>.
        <li style="margin: 8px 0;">[06.2022] Our paper (on <a href="https://www.sciencedirect.com/science/article/pii/S2213158221001595">Neuroimage:clinical</a>) is reported by <a href="https://www.nature.com/articles/s41582-021-00543-3">Nature Reviews Neurology</a>.
        <li style="margin: 8px 0;">[05.2022] One paper (First Author) is accepted by <a href="https://ieeexplore.ieee.org/abstract/document/9897454">ICIP-2022</a>.
        <li style="margin: 8px 0;">[04.2022] One paper (First Author) is accepted by <a href="https://proceedings.mlr.press/v172/yang22a/yang22a.pdf">MIDL-2022 </a> (oral).
        <li style="margin: 8px 0;">[03.2022] We report the solution of MICCAI-QUBIQ-2021 challenge in <a href="https://link.springer.com/chapter/10.1007/978-3-031-09002-8_50">MICCAI-2021-BrainLes</a>.
        <li style="margin: 8px 0;">[11.2021] I win the <strong>Top</strong> in the <a href="https://contest.taop.qq.com/">Tecent AIMIS 2021 challenge</a> on brain age prediction.
        <li style="margin: 8px 0;">[10.2021] Our team achieved the <strong>Top</strong> in <a href="https://qubiq21.grand-challenge.org/">MICCAI-QUBIQ-2021 challenge</a> and <a href="https://valdo.grand-challenge.org/">MICCAI-VALDO 2021 challenge</a>.
        <li style="margin: 8px 0;">[06.2021] One paper is accepted by <a href="https://www.sciencedirect.com/science/article/pii/S2213158221001595">Neuroimage:clinical</a> .
        <li style="margin: 8px 0;">[12.2020] I become a project manager for the SuWen project of PengCheng Lab.
        <li style="margin: 8px 0;">[10.2020] I win the <strong>Runner-up</strong> in multi-center AD diganosis challenge.
        <li style="margin: 8px 0;">[09.2020] I win the <strong>Runner-up</strong> in <a href="https://qubiq.grand-challenge.org/">MICCAI-QUBIQ-2020 challenge</a>.
        </ul>
        </p>
        <br> 
        <heading>Research</heading><p>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="12">
        <tbody>
        <tr>
        <td width="25%"><img src="files/paper11-mia.png" width="220">
          </td>
         <td width="75%" valign="middle">
         <papertitle>CReg-KD: Model Refinement via Confidence Regularized Knowledge Distillation for Brain Imaging</papertitle>
           <br><U>Yanwu Yang</U>, Xutao Guo, Chenfei Ye, Yang Xiang*, and Ting Ma*<br>
              <em> Medical Image Analysis (MedIA)</em>, 2023.<br>
           <a href="http://podismine.github.io/files/paper11-mia.pdf">Paper</a> / 
           <a href="https://github.com/podismine/CReg-KD">Code</a> /
          </td>
        </tr>
        <tr>
        <td width="25%"><img src="files/streamRF.png" width="200">
          </td>
         <td width="75%" valign="middle">
         <papertitle>Streaming Radiance Fields for 3D Video Synthesis</papertitle>
           <br>Lingzhi Li, Zhen Shen, Zhongshu Wang, <U>Li Shen</U> and Ping Tan<br>
              <em> Advances in Neural Information Processing Systems (NeurIPS)</em>, 2022.<br>
           <a href="https://openreview.net/pdf?id=oMhmv3hLOF2">Paper</a> / 
           <a href="https://arxiv.org/pdf/2210.14831.pdf">ArXiv</a> /
           <a href="https://github.com/AlgoHunt/StreamRF">Code</a> /
              <a href="bibtex/li_streamRF.bib" download="li_streamRF.bib">Bibtex</a>
          </td>
        </tr>
        <tr>
        <td width="25%"><img src="files/talkhead.png" width="200">
          </td>
         <td width="75%" valign="middle">
         <papertitle>Depth-Aware Generative Adversarial Network for Talking Head Video Generation</papertitle>
           <br>Fa-Ting Hong, Longhao Zhang, <U>Li Shen</U> and Dan Xu<br>
              <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022.<br>
           <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Hong_Depth-Aware_Generative_Adversarial_Network_for_Talking_Head_Video_Generation_CVPR_2022_paper.pdf">Paper</a> / 
           <a href="https://arxiv.org/pdf/2203.06605.pdf">ArXiv</a> /
           <a href="https://github.com/harlanhong/CVPR2022-DaGAN">Code</a> /
              <a href="bibtex/hong_talkhead.bib" download="hong_talkhead.bib">Bibtex</a>
          </td>
        </tr>
        <tr>
        <td width="25%"><img src="files/MARL.png" width="160">
          </td>
         <td width="75%" valign="middle">
         <papertitle>Learning Meta Representations for Agents in Multi-Agent Reinforcement Learning</papertitle>
           <br>Shenao Zhang, <U>Li Shen</U>, Lei Han and Li Shen<br>
              <em>Gamification and Multiagent Solutions Workshop, ICLR</em>, 2022.<br>
           <a href="https://openreview.net/pdf?id=SbNlwsq1peq">Paper</a> / 
           <a href="https://arxiv.org/pdf/2108.12988.pdf">ArXiv</a> /
              <a href="bibtex/zhang_marl.bib" download="zhang_marl.bib">Bibtex</a>
          </td>
        </tr>
        <tr>
        <td width="25%"><img src="files/stRA.png" width="200">
          </td>
         <td width="75%" valign="middle">
         <papertitle>Structure-Regularized Attention for Deformable Object Representation</papertitle>
           <br>Shenao Zhang, <U>Li Shen</U>, Zhifeng Li and Wei Liu<br>
              <em>Object Representations for Learning and Reasoning Workshop, NeurIPS</em>, 2020.<br>
           <a href="https://arxiv.org/pdf/2106.06672.pdf">Paper</a> / 
           <a href="https://arxiv.org/pdf/2106.06672.pdf">ArXiv</a> /
              <a href="https://github.com/shenao-zhang/StRA">Code</a> /
              <a href="bibtex/zhang_stRA.bib" download="zhang_stRA.bib">Bibtex</a>
          </td>
          </tr>
        <tr>
        <td width="25%"><img src="files/alphagan.png" width="150">
          </td>
         <td width="75%" valign="middle">
         <papertitle>AlphaGAN: Fully Differentiable Architecture Search for Generative Adversarial Networks</papertitle>
           <br>Yuesong Tian, Li Shen <U>Li Shen</U>, Guinan Su, Zhifeng Li and Wei Liu<br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 2021.<br>
           <a href="https://ieeexplore.ieee.org/document/9496081">Paper</a> / 
           <a href="https://arxiv.org/pdf/2006.09134.pdf">ArXiv</a> /
              <a href="https://github.com/yuesongtian/AlphaGAN">Code</a> /
              <a href="bibtex/tian_pami21.bib" download="tian_pami21.bib">Bibtex</a>
          </td>
          </tr>
        <tr>
        <td width="25%"><img src="files/mileNas.png" width="150">
          </td>
         <td width="75%" valign="middle">
         <papertitle>Milenas: Efficient neural architecture search via mixed-level reformulation</papertitle>
           <br>Chaoyang He, Haishan Ye, <U>Li Shen</U>, and Tong Zhang<br>
              <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020.<br>
           <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/He_MiLeNAS_Efficient_Neural_Architecture_Search_via_Mixed-Level_Reformulation_CVPR_2020_paper.pdf">Paper</a> / 
           <a href="https://arxiv.org/pdf/2003.12238.pdf">ArXiv</a> /
              <a href="https://github.com/chaoyanghe/MiLeNAS">Code</a> /
              <a href="bibtex/he_cvpr20.bib" download="he_cvpr20.bib">Bibtex</a>
          </td>
          </tr>
        <tr> 
        <td width="25%"><img src="files/se_journal_icon.png" width="200">
          </td>
         <td width="75%" valign="middle"> 
         <papertitle>Squeeze-and-Excitation Networks</papertitle>
           <br>Jie Hu, <U>Li Shen</U>, Samuel Albanie, Gang Sun, and Enhua Wu<br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 2019.<br>
           This paper is an extension of prior work 
           <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf">SENets</a>. <br>
           <a href="https://ieeexplore.ieee.org/document/8701503">Paper</a> / <a href="https://arxiv.org/pdf/1709.01507.pdf">ArXiv</a> /
              <a href="https://github.com/hujie-frank/SENet">Code &amp; Model</a> /
              <a href="bibtex/hu_pami19.bib" download="hu_pami19.bib">Bibtex</a>
          </td>
          </tr>
          <tr> 
        <td width="25%"><img src="files/ge_icon.png" width="210">
          </td>
         <td width="75%" valign="middle"> 
         <papertitle>Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks</papertitle>
          <br>Jie Hu*, <U>Li Shen</U>*, Samuel Albanie*, Gang Sun, and Andrea Vedaldi<br>
              <em>Advances in Neural Information Processing Systems</em> (NeurIPS), 2018.<br>
           <a href="https://papers.nips.cc/paper/8151-gather-excite-exploiting-feature-context-in-convolutional-neural-networks.pdf">Paper</a> / <a href="https://arxiv.org/pdf/1810.12348.pdf">ArXiv</a> /
              <a href="https://github.com/hujie-frank/GENet">Code &amp; Model</a> / <a href="bibtex/hu_nips18.bib" download="hu_nips18.bib">Bibtex</a>
          </td>
          </tr>
        <tr> 
        <td width="25%" align="center"><img src="files/comparator_icon.png" width="190">
          </td>
         <td width="75%" valign="middle"> 
              <papertitle>Comparator Networks</papertitle>
              <br>Weidi Xie, <U>Li Shen</U>, and Andrew Zisserman
              <br>
              <em>European Conference on Computer Vision (ECCV)</em>, 2018.
              <br>
              <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Weidi_Xie_Comparator_Networks_ECCV_2018_paper.pdf">Paper</a> / <a href="https://arxiv.org/abs/1807.11440">ArXiv</a> / <a href="bibtex/xie_eccv18.bib" download="xie_eccv18.bib">Bibtex</a>
          </tr>
          <tr> 
        <td width="25%"><img src="files/se_icon.png" width="220">
          </td>
         <td width="75%" valign="middle"> 
              <papertitle>Squeeze-and-Excitation Networks</papertitle>
              <br>Jie Hu*, <U>Li Shen</U>*, and Gang Sun<br>
              <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2018 <strong>(Oral)</strong>.<br>
           <strong>Winner at the <a href="http://image-net.org/challenges/LSVRC/2017/results">ILSVRC 2017</a> Image Classification</strong>.<br>
           <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf">Paper</a> /
              <a href="https://github.com/hujie-frank/SENet">Code &amp; Model</a> /
              <a href="bibtex/hu_cvpr18.bib" download="hu_cvpr18.bib">Bibtex</a>
          </td>
          </tr>
        <tr> 
        <td width="25%"><img src="files/vggface2_icon.png" width="200">
          </td>
         <td width="75%" valign="middle"> 
          <papertitle>Vggface2: A dataset for recognising faces across pose and age</papertitle>
              <br>Qiong Cao, <U>Li Shen</U>, Weidi Xie, Omkar M. Parkhi, and Andrew Zisserman<br>
              <em>Conference on Automatic Face & Gesture Recognition (FG)</em>, 2018 <strong>(Oral)</strong>.<br>
          <a href="https://ieeexplore.ieee.org/document/8373813">Paper</a> / <a href="https://arxiv.org/pdf/1710.08092">ArXiv</a> /
              <a href="https://github.com/ox-vgg/vgg_face2">Code &amp; Model</a> / 
           <a href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/">Project</a> / 
              <a href="bibtex/cao_fg18.bib" download="cao_fg18.bib">Bibtex</a>
          </td>
          </tr>
        <tr> 
        <td width="25%"><img src="files/relaybp_icon.png" width="200">
          </td>
         <td width="75%" valign="middle"> 
          <papertitle>Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks</papertitle>
              <br> <U>Li Shen</U>, Zhouchen Lin, and Qingming Huang<br>
              <em>European conference on computer vision (ECCV)</em>, 2016.<br>
           <strong>Winner at the <a href="http://image-net.org/challenges/LSVRC/2015/results">ILSVRC 2015</a> Scene Classification</strong>.<br>
           <a href="https://rd.springer.com/chapter/10.1007/978-3-319-46478-7_29">Paper / 
           <a href="https://arxiv.org/pdf/1512.05830.pdf">ArXiv</a> /
              <a href="https://github.com/lishen-shirley/Places2-CNNs">Model</a> / 
              <a href="bibtex/shen_eccv16.bib" download="shen_eccv16.bib">Bibtex</a>
          </td>
          </tr>
        <tr> 
        <td width="25%"><img src="files/skeleton_lstm_icon.png" width="200">
          </td>
         <td width="75%" valign="middle"> 
          <papertitle>Co-occurrence Feature Learning for Skeleton Based Action Recognition Using Regularized Deep LSTM Networks</papertitle>
              <br>Wentao Zhu, Cuiling Lan, Junliang Xing, Wenjun Zeng, Yanghao Li, <U>Li Shen</U>, and Xiaohui Xie<br>
              <em>Thirtieth AAAI Conference on Artificial Intelligence (AAAI)</em>, 2016.<br>
           <a href="https://dl.acm.org/citation.cfm?id=3016423">Paper / 
           <a href="https://arxiv.org/pdf/1603.07772.pdf">ArXiv</a> / 
              <a href="bibtex/zhu_aaai16.bib" download="zhu_aaai16.bib">Bibtex</a>
          </td>
          </tr>
        <tr> 
        <td width="25%"><img src="files/mlddl_journal_icon.png" width="200">
          </td>
         <td width="75%" valign="middle"> 
          <papertitle>Multi-Level Discriminative Dictionary Learning with Application to Large Scale Image Classification</papertitle>
           <br><U>Li Shen</U>, Gang Sun, Qingming Huang, Shuhui Wang, Zhouchen Lin, and Enhua Wu<br>
              <em>IEEE Transactions on Image Processing</em>, 2015.<br>
           <a href="http://www.jdl.link/doc/2011/20161191011898744_tip15-shen.pdf">Paper / 
              <a href="bibtex/shen_tip15.bib" download="shen_tip15.bib">Bibtex</a>
          </td>
          </tr>
        <tr> 
        <td width="25%"><img src="files/share_icon.png" width="190">
          </td>
         <td width="75%" valign="middle"> 
          <papertitle>Adaptive Sharing for Image Classification</papertitle>
           <br><U>Li Shen</U>, Gang Sun, Zhouchen Lin, Qingming Huang, and Enhua Wu<br>
              <em>International Joint Conference on Artificial Intelligence (IJCAI)</em>, 2015.<br>
           <a href="https://www.ijcai.org/Proceedings/15/Papers/309.pdf">Paper / 
              <a href="bibtex/shen_ijcai15.bib" download="shen_ijcai15.bib">Bibtex</a>
          </td>
          </tr>
        <tr> 
        <td width="25%"><img src="files/mlddl_icon.png" width="190">
          </td>
         <td width="75%" valign="middle"> 
          <papertitle>Multi-Level Discriminative Dictionary Learning towards Hierarchical Visual Categorization</papertitle>
           <br><U>Li Shen</U>, Shuhui Wang, Gang Sun, Shuqiang Jiang, and Qingming Huang<br>
              <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2013.<br>
           <a href="http://openaccess.thecvf.com/content_cvpr_2013/papers/Shen_Multi-level_Discriminative_Dictionary_2013_CVPR_paper.pdf">Paper / 
              <a href="bibtex/shen_cvpr13.bib" download="shen_cvpr13.bib">Bibtex</a>
          </td>
          </tr>
  </tbody></table>
  <br>
  <br>
  <heading> Academic Competitions </heading>
          <ul style="list-style-type:circle;">
          <li style="margin: 8px 0;">
          Winner at the <a href="http://image-net.org/challenges/LSVRC/2017/results">ImageNet LSVRC 2017</a> Image Classification (<a href="http://image-net.org/challenges/beyond_ilsvrc">CVPR17 Workshop</a>). </li>
          <li style="margin: 8px 0;">
          Winner at the <a href="http://image-net.org/challenges/LSVRC/2015/results">ImageNet LSVRC 2015</a> Scene Classification (<a href="http://image-net.org/challenges/ilsvrc+mscoco2015">ICCV15 Workshop</a>). </li>
          </ul>
          <br>
          <br>
          <br>
         <p align="right">Last Update: March 28, 2023</p>
         <p align="right">Published with <a href="https://pages.github.com">GitHub Pages</a></p>
    </tbody>
    </table>
          <br>
          <br>
</body></html>
